{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c38df0cd3f7fc2de899019ae9e982e36",
     "grade": false,
     "grade_id": "cell-f5e46023398b0aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> April 10, 2023 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Neural machine translation with transformers.\n",
    "\n",
    "The goal of this exerscise is to get familiar with a transformer model, which was introduced in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "We base our code on the [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post.\n",
    "Module `transformer.py` contains some useful modules from that blog post. We recommend you to use those modules when we state so in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cc4d569dc32e40fe066146a07b7c7b7",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformer as tr\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is ../data\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbca8fe9cf0cb1cb20dd200e23cfcb0",
     "grade": false,
     "grade_id": "cell-44cf6f3242607cde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d91ed28b2953b2c5909cd59854eed573",
     "grade": false,
     "grade_id": "cell-1f1e529682d7ce6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We use the same translation dataset as in the RNN exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16144210eada0de3091243f95367f7c2",
     "grade": false,
     "grade_id": "cell-94d57799bcd1786b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:\n",
      " as word indices:  tensor([   6,   11,   14, 2445,  101, 1217, 3891,    5,    1])\n",
      " as string:  je suis en faveur de ta proposition . EOS\n",
      "Target sentence:\n",
      " as word indices:  tensor([   2,   16,  102, 2339,  525,  357, 2447,    4,    1])\n",
      " as string:  i am in favor of your proposal . EOS\n"
     ]
    }
   ],
   "source": [
    "# Translation data\n",
    "from data import TranslationDataset, SOS_token, EOS_token, MAX_LENGTH\n",
    "trainset = TranslationDataset(data_dir, train=True)\n",
    "\n",
    "src_seq, tgt_seq = trainset[np.random.choice(len(trainset))]\n",
    "print('Source sentence:')\n",
    "print(' as word indices: ', src_seq)\n",
    "print(' as string: ', ' '.join(trainset.input_lang.index2word[i.item()] for i in src_seq))\n",
    "\n",
    "print('Target sentence:')\n",
    "print(' as word indices: ', tgt_seq)\n",
    "print(' as string: ', ' '.join(trainset.output_lang.index2word[i.item()] for i in tgt_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ecf9ce3f0b267a5c738cc20ab96250",
     "grade": false,
     "grade_id": "cell-86482ed71ea81ed3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "Next we prepare a custom data loader which puts sequences of varying lengths in one tensor. We do so by using a custom `collate_fn` (see the description of the `collate_fn` argument of [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)).\n",
    "\n",
    "Our collate function combines source sequences in one tensor `src_seqs` with extra values (at the end) filled with `PADDING_VALUE=0`. To tell the transformer which elements are padded, we also need to compute the mask `src_mask`.\n",
    "\n",
    "The function also combines target sequences in one tensor `tgt_seqs` but it does it a bit differently:\n",
    "* The resulting tensor is of shape `(max_tgt_seq_length+1, batch_size)`, where `max_tgt_seq_length` is the length of the longest target sequence in the mini-batch.\n",
    "* The first element of each sequence in the resulting tensor is `SOS_token`.\n",
    "* The remaining elements are filled similarly to the source sequences with extra values (at the end) filled with `PADDING_VALUE=0`.\n",
    "\n",
    "We will use tensor `tgt_seqs[:-1]` as inputs of the transformer decoder and `tgt_seqs[1:]` as the targets for the model (decoder) outputs. The `SOS_token` is needed to predict the first word in the output sequence in the corresponding (first) location of the decoder output.\n",
    "\n",
    "Your task is to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "364bdf1c40bbfd85d74a002df498033f",
     "grade": false,
     "grade_id": "cell-77f035ba6ccb554d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d80be252d280552db0469e868f6c2ed0",
     "grade": false,
     "grade_id": "collate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (src_seq, tgt_seq):\n",
    "          src_seq is of shape (src_seq_length)\n",
    "          tgt_seq is of shape (tgt_seq_length)\n",
    "\n",
    "    Returns:\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): LongTensor of padded source sequences.\n",
    "      src_mask of shape (max_src_seq_length, batch_size): BoolTensor (tensor with boolean elements) indicating which\n",
    "          elements of the src_seqs tensor should be ignored in computations: True values in src_mask correspond\n",
    "          to padding values in src_seqs.\n",
    "      tgt_seqs of shape (max_tgt_seq_length+1, batch_size): LongTensor of padded target sequences.\n",
    "    \"\"\"\n",
    "    src_seqs = [pair[0] for pair in list_of_samples]\n",
    "    tgt_seqs = [pair[1] for pair in list_of_samples]\n",
    "    \n",
    "    src_seqs = pad_sequence(src_seqs, batch_first=False, padding_value=PADDING_VALUE)\n",
    "\n",
    "    #sos_tokens = torch.full((tgt_seqs.shape[0], 1), SOS_token, dtype=torch.long)\n",
    "    tgt_seqs = [torch.cat([torch.tensor([SOS_token]), seq]) for seq in tgt_seqs]\n",
    "    tgt_seqs = pad_sequence(tgt_seqs, batch_first=False, padding_value=PADDING_VALUE)\n",
    " \n",
    "    #src_seqs = src_seqs.t()\n",
    "    #tgt_seqs = tgt_seqs.t()\n",
    "    src_mask = src_seqs == PADDING_VALUE\n",
    "    \n",
    "    return src_seqs, src_mask, tgt_seqs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ce1cbec7ff20d420fc4a68362c5e481",
     "grade": false,
     "grade_id": "cell-a99b4cfa1fc559f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([2, EOS_token]), torch.LongTensor([3, 4, EOS_token])),\n",
    "        (torch.LongTensor([6, 7, EOS_token]), torch.LongTensor([9, EOS_token])),\n",
    "    ]\n",
    "    src_seqs, src_mask, tgt_seqs = collate(pairs)\n",
    "    assert src_seqs.dtype == torch.long, f\"Wrong src_seqs.dtype: {src_seqs.dtype}\"\n",
    "    assert src_seqs.shape == torch.Size([3, 2]), f\"Wrong src_seqs.shape: {src_seqs.shape}\"\n",
    "\n",
    "    assert tgt_seqs.dtype == torch.long, f\"Wrong tgt_seqs.dtype: {tgt_seqs.dtype}\"\n",
    "    assert tgt_seqs.shape == torch.Size([4, 2]), f\"Wrong tgt_seqs.shape: {tgt_seqs.shape}\"\n",
    "    assert (tgt_seqs[0] == torch.empty(2, dtype=torch.long).fill_(SOS_token)).all(), \"Target sequences should start with SOS_token.\"\n",
    "    \n",
    "    assert src_mask.dtype == torch.bool, f\"Wrong src_mask.dtype: {src_mask.dtype}\"\n",
    "    assert src_mask.shape == src_seqs.shape, f\"Wrong src_mask.shape: {src_mask.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c7e0bd6756641450a7b09587483ecdc",
     "grade": true,
     "grade_id": "test_collate",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_seqs:\n",
      " tensor([[2, 6],\n",
      "        [1, 7],\n",
      "        [0, 1]])\n",
      "src_mask:\n",
      " tensor([[False, False],\n",
      "        [False, False],\n",
      "        [ True, False]])\n",
      "tgt_seqs:\n",
      " tensor([[0, 0],\n",
      "        [3, 9],\n",
      "        [4, 1],\n",
      "        [1, 0]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests collate()\n",
    "def test_collate():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([2, EOS_token]), torch.LongTensor([3, 4, EOS_token])),\n",
    "        (torch.LongTensor([6, 7, EOS_token]), torch.LongTensor([9, EOS_token])),\n",
    "    ]\n",
    "    src_seqs, src_mask, tgt_seqs = collate(pairs)\n",
    "    #src_seqs, src_mask, tgt_seqs = src_seqs[:, [1, 0]], src_mask[:, [1, 0]], tgt_seqs[:, [1, 0]]\n",
    "    print('src_seqs:\\n', src_seqs)\n",
    "    print('src_mask:\\n', src_mask)\n",
    "    print('tgt_seqs:\\n', tgt_seqs)\n",
    "    expected_src_seqs = torch.tensor([\n",
    "        [2,         6],\n",
    "        [EOS_token, 7],\n",
    "        [0,         EOS_token]\n",
    "    ])\n",
    "    expected_src_mask = torch.tensor([\n",
    "        [False, False],\n",
    "        [False, False],\n",
    "        [ True, False]\n",
    "    ])\n",
    "    expected_tgt_seqs = torch.tensor([\n",
    "        [0,         0],\n",
    "        [3,         9],\n",
    "        [4,         EOS_token],\n",
    "        [EOS_token, 0]\n",
    "    ])\n",
    "    \n",
    "    assert ((\n",
    "        (src_seqs == expected_src_seqs).all()\n",
    "         and (src_mask == expected_src_mask).all()\n",
    "         and (tgt_seqs == expected_tgt_seqs).all()\n",
    "        ) or (\n",
    "        (src_seqs == expected_src_seqs[:, [1, 0]]).all()\n",
    "         and (src_mask == expected_src_mask[:, [1, 0]]).all()\n",
    "         and (tgt_seqs == expected_tgt_seqs[:, [1, 0]]).all()\n",
    "        )\n",
    "    ), \"Wrong outputs of collate.\"\n",
    "    print('Success')\n",
    "\n",
    "test_collate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f103d72cfd56ec7f6e42872740b6f395",
     "grade": false,
     "grade_id": "cell-e0a6bbaf21ae2a36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time\n",
    "batch_size=64\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d04f108eaf1d8095c09d0b440b75a212",
     "grade": false,
     "grade_id": "cell-847fb19371903dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create test set\n",
    "testset = TranslationDataset(data_dir, train=False)\n",
    "testloader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6627cb26c83634c853b5749710275900",
     "grade": false,
     "grade_id": "cell-3f6dfc8dc7015270",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dae02152084b2cb3f988c5da5c0e588",
     "grade": false,
     "grade_id": "cell-63be98428fcdc0b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Encoder block\n",
    "\n",
    "<img src=\"encoder_block.png\" width=150 style=\"float: right;\">\n",
    "\n",
    "We first implement one block of the transformer encoder (see the figure on the right).\n",
    "* We recommend you to use layers available in PyTorch:\n",
    "  * [nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm) to implement the `Norm` layer in the figure\n",
    "  * [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) to implement dropout\n",
    "  * [nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention) to implement `Multi-Head Attention`.\n",
    "\n",
    "* `Feedforward` is simply an MLP processing each position (each element of the source sequence) independently. The exact implementation of the MLP is not tested in this notebook. We used an MLP with:\n",
    "  * one hidden layer with `n_hidden` neurons\n",
    "  * a dropout and ReLU activation after the hidden layer\n",
    "  * an output layer with `n_features` outputs.\n",
    "\n",
    "* In two places where skip connections are used, we apply dropout on the main path, combine the main path with the skip connection and then apply layer normalization. This order is slightly different to the [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) code.\n",
    "\n",
    "Hints:\n",
    "* **We recommend you to test that the padded values of the input sequence do not affect the outputs in the positions that correspond to non-padded values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d490069146fdfa32112d33744eb59d1",
     "grade": false,
     "grade_id": "EncoderBlock",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features.\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block.\n",
    "          dropout: Dropout rate after the first layer of the MLP and in two places on the main path (before\n",
    "                   combining the main path with a skip connection).\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(n_features, n_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_features)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (max_seq_length, batch_size, n_features): Input sequences.\n",
    "          mask of shape (max_seq_length, batch_size): BoolTensor indicating which elements of the input\n",
    "              sequences should be ignored (True values correspond to ignored elements in x).\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequences.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        #x[mask,:] = 0\n",
    "        #y, _ = self.multihead_attention(x, x, x)\n",
    "        y, _ = self.multihead_attention(x,x,x, key_padding_mask=mask.transpose(0,1))\n",
    "        y = self.dropout1(y) + x\n",
    "        y = self.norm1(y)\n",
    "        z = self.feed_forward(y)\n",
    "        z = self.dropout2(z) + y\n",
    "        z = self.norm2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0146983a24b7930bef1ba41bd0c3d4e",
     "grade": false,
     "grade_id": "cell-67f5cb6fdfecf7d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_EncoderBlock_shapes():\n",
    "    encoder_block = EncoderBlock(n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    x = torch.tensor([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 0],\n",
    "        [6, 0],\n",
    "    ]).float().view(4, 2, 1).repeat(1, 1, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "    outputs = encoder_block(x, mask)\n",
    "    assert outputs.shape == torch.Size([4, 2, 16]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_EncoderBlock_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54926fc1f539f5d07dcba0047f2b9c8b",
     "grade": true,
     "grade_id": "test_EncoderBlock",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs1[:2,1,:]:\n",
      " tensor([[-1.1817, -1.1815,  1.6143, -0.3142, -1.5665, -1.5348,  1.3323,  0.1504,\n",
      "          0.7789, -0.4173, -0.1232,  1.3128,  0.7438, -0.5488,  0.2630,  0.6726],\n",
      "        [-1.0903, -1.1195,  1.5669, -0.1684, -1.5329, -1.5659,  1.3295, -0.1643,\n",
      "          0.7383, -0.4549, -0.1497,  1.4242,  0.8505, -0.6361,  0.2941,  0.6784]])\n",
      "outputs2[:2,1,:]:\n",
      " tensor([[-1.1784, -1.1772,  1.6101, -0.3040, -1.5654, -1.5375,  1.3350,  0.1362,\n",
      "          0.7789, -0.4191, -0.1271,  1.3184,  0.7495, -0.5527,  0.2599,  0.6735],\n",
      "        [-1.1302, -1.1417,  1.5814, -0.2223, -1.5475, -1.5598,  1.3382, -0.0309,\n",
      "          0.7607, -0.4430, -0.1461,  1.3789,  0.8085, -0.6001,  0.2730,  0.6807]])\n",
      "outputs3[:2,1,:]:\n",
      " tensor([[-1.1784, -1.1772,  1.6101, -0.3040, -1.5654, -1.5375,  1.3350,  0.1362,\n",
      "          0.7789, -0.4191, -0.1271,  1.3184,  0.7495, -0.5527,  0.2599,  0.6735],\n",
      "        [-1.1302, -1.1417,  1.5814, -0.2223, -1.5475, -1.5598,  1.3382, -0.0309,\n",
      "          0.7607, -0.4430, -0.1461,  1.3789,  0.8085, -0.6001,  0.2730,  0.6807]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests EncoderBlock\n",
    "# Check that the signal does not propagate from the padded elements\n",
    "import unittest.mock\n",
    "\n",
    "def no_dropout(x, *args, **kwargs):\n",
    "    return x\n",
    "\n",
    "@unittest.mock.patch('torch.nn.functional.dropout', no_dropout)  # .eval() does not disable F.dropout()\n",
    "def test_EncoderBlock():\n",
    "    with torch.no_grad():\n",
    "        encoder_block = EncoderBlock(n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "        x = torch.tensor([\n",
    "            [1, 2],\n",
    "            [3, 4],\n",
    "            [5, 0],\n",
    "            [6, 0],\n",
    "        ]).float().view(4, 2, 1).repeat(1, 1, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "        mask = torch.tensor([\n",
    "            [0, 0],\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [0, 1],\n",
    "        ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "        outputs1 = encoder_block(x, mask)\n",
    "        print('outputs1[:2,1,:]:\\n', outputs1[:2,1,:])\n",
    "\n",
    "        # Modify non-padded values\n",
    "        x[:2,1,:] = x[:2,1,:] + 1\n",
    "        outputs2 = encoder_block(x, mask)\n",
    "        print('outputs2[:2,1,:]:\\n', outputs2[:2,1,:])\n",
    "        diff = outputs2[:2,1,:] - outputs1[:2,1,:]\n",
    "        assert torch.max(torch.abs(diff)) > 0, \"Some elements of the source sequence do not affect the output.\"\n",
    "        \n",
    "        # Modify padded values\n",
    "        x[2:,1,:] = x[2:,1,:] + 1\n",
    "        outputs3 = encoder_block(x, mask)\n",
    "        print('outputs3[:2,1,:]:\\n', outputs3[:2,1,:])\n",
    "        diff = outputs3[:2,1,:] - outputs2[:2,1,:]\n",
    "        assert torch.max(torch.abs(diff)) == 0, \"Padding values affect the output.\"\n",
    "        \n",
    "        print('Success')\n",
    "\n",
    "test_EncoderBlock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1506a5cd79b342fd5325431e939fc59",
     "grade": false,
     "grade_id": "cell-e2776b50381263dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"encoder.png\" width=200 style=\"float: right;\">\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The encoder is a stack of the following blocks:\n",
    "* Embedding of words (please use [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding))\n",
    "* Positional encoding (please use `tr.PositionalEncoding` from the attached module)\n",
    "* `n_blocks` of the `EncoderBlock` modules.\n",
    "\n",
    "Notes:\n",
    "* Provided implementation of `tr.PositionalEncoding` is the same as in [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post. See the blog post for more detail.\n",
    "* Our longest sequences have length `MAX_LENGTH`, this is the value that you can use when you specify `PositionalEncoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50bf400c4630234e0437a7c18c884abd",
     "grade": false,
     "grade_id": "Encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_blocks, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_vocab_size: Number of words in the source vocabulary.\n",
    "          n_blocks: Number of EncoderBlock blocks.\n",
    "          n_features: Number of features to be used for word embedding and further in all layers of the encoder.\n",
    "          n_heads: Number of attention heads inside the EncoderBlock.\n",
    "          n_hidden: Number of hidden units in the Feedforward block of EncoderBlock.\n",
    "          dropout: Dropout level used in EncoderBlock.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, n_features)\n",
    "        self.positional = tr.PositionalEncoding(n_features, dropout, MAX_LENGTH)\n",
    "        module = EncoderBlock(n_features, n_heads, n_hidden, dropout)\n",
    "        self.blocks = nn.ModuleList([copy.deepcopy(module) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (max_seq_length, batch_size): LongTensor with the input sequences.\n",
    "          mask of shape (max_seq_length, batch_size): BoolTensor indicating which elements should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          z of shape (max_seq_length, batch_size, n_features): Encoded input sequences.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84a652b200c4f5bbfa06a5c36071dd3c",
     "grade": false,
     "grade_id": "cell-01134ebab1f5117e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Encoder_shapes():\n",
    "    encoder = Encoder(src_vocab_size=10, n_blocks=1, n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    x = torch.tensor([\n",
    "        [SOS_token,     SOS_token],\n",
    "        [        3,             4],\n",
    "        [        5,     EOS_token],\n",
    "        [        6, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE],\n",
    "    ])  # (max_seq_length, batch_size)\n",
    "\n",
    "    mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "    outputs = encoder(x, mask)\n",
    "    assert outputs.shape == torch.Size([5, 2, 16]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c59d3e8bf698511a855df8b7fbc04491",
     "grade": false,
     "grade_id": "cell-102038af7faba64b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d34b280c27c18042a00b922f9e47d40",
     "grade": false,
     "grade_id": "cell-3133e50590987e56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Subsequent mask\n",
    "\n",
    "In the training loop, we will use target sequences (starting with `SOS_token`) as inputs of the decoder. By doing that, we make it possible for the decoder to use previously decoded words when predicting probabilities of the next word. This idea is similar to the way decoding was done in the RNN exercise. However, the computations are parallelized in the transformer decoder, and the probabilities of each word in the target sequence are produced by doing one pass through the decoder.\n",
    "\n",
    "During decoding, we need to make sure that when we compute the probability of the next word, we only use preceding and not subsequent words. In transformers, this is done by providing a mask which tells which elements should be used or ignored when producing the output. The following function produces this kind of mask.\n",
    "\n",
    "The $i$-th row in the produced mask says which of the input elements should be used to compute the $i$-th element of the output:\n",
    "* `0`: the corresponding element of the input sequence can be used.\n",
    "* `-inf`: the corresponding element of the input sequence cannot be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c91dbc90b6c803baeffb280bbf39324b",
     "grade": false,
     "grade_id": "cell-c2dfee9a9f2d674e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1).float()\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20d6d1ff1df0c0609a1cfc1954e4e94c",
     "grade": false,
     "grade_id": "cell-ec36a7f1884b54f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25cfb7a90f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a typical mask that we need to use while decoding\n",
    "mask = subsequent_mask(10)\n",
    "print(mask)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc8d9b4f4abdbff54da696cb4047223d",
     "grade": false,
     "grade_id": "cell-c26c4a8fecf141bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"decoder_block.png\" width=150 style=\"float: right;\">\n",
    "\n",
    "## Decoder block\n",
    "\n",
    "Next we implement one block of the transformer decoder (see the figure on the right).\n",
    "* We recommend you to use layers available in PyTorch:\n",
    "  * [nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm) to implement the `Norm` layer in the figure\n",
    "  * [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) to implement dropout\n",
    "  * [nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention) to implement `Multi-Head Attention`.\n",
    "\n",
    "* `Feedforward` is simply an MLP processing each position (each element of the source sequence) independently. The exact implementation of the MLP is not tested in this notebook. We used an MLP with:\n",
    "  * one hidden layer with `n_hidden` neurons\n",
    "  * a dropout and ReLU activation after the hidden layer\n",
    "  * an output layer with `n_features` outputs.\n",
    "\n",
    "* In three places where skip connections are used, we applied dropout on the main path, combined the main path with the skip connection and then applied layer normalization. This order is slightly different to the [Annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) code.\n",
    "\n",
    "Notes:\n",
    "* The first attention block is self-attention when query, key and value inputs are same. The second attention block uses the encoded `z` values as keys and values, and the outputs of the previous layer as query.\n",
    "* **We recommend you to test that the subsequent values of the input sequence do not affect the outputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c362c463285842998fa3f021db9c3fd4",
     "grade": false,
     "grade_id": "DecoderBlock",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_features: Number of input and output features.\n",
    "          n_heads: Number of attention heads in the Multi-Head Attention.\n",
    "          n_hidden: Number of hidden units in the Feedforward (MLP) block.\n",
    "          dropout: Dropout rate after the first layer of the MLP and in three places on the main path (before\n",
    "                   combining the main path with a skip connection).\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(n_features, n_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.src_attn = nn.MultiheadAttention(n_features, n_heads)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_features)\n",
    "        )\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(n_features)\n",
    "\n",
    "    def forward(self, y, z, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y of shape (max_tgt_seq_length, batch_size, n_features): Transformed target sequences used as the inputs\n",
    "              of the block.\n",
    "          z of shape (max_src_seq_length, batch_size, n_features): Encoded source sequences (outputs of the\n",
    "              encoder).\n",
    "          src_mask of shape (max_src_seq_length, batch_size): BoolTensor indicating which elements of the\n",
    "             encoded source sequences should be ignored.\n",
    "          tgt_mask of shape (max_tgt_seq_length, max_tgt_seq_length): Subsequent mask to ignore subsequent\n",
    "             elements of the target sequences in the inputs. The rows of this matrix correspond to the output\n",
    "             elements and the columns correspond to the input elements.\n",
    "        \n",
    "        Returns:\n",
    "          out of shape (max_seq_length, batch_size, n_features): Output tensor.\n",
    "\n",
    "        Note: All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "        \"\"\"\n",
    "        attn1, _ = self.self_attn(y, y, y, attn_mask=tgt_mask)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        attn1 = self.norm1(attn1 + y)\n",
    "        key_mask = src_mask.transpose(0, 1)\n",
    "        attn2, _ = self.src_attn(attn1, z, z, key_padding_mask=key_mask)#, attn_mask=tgt_mask)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        attn2 = self.norm2(attn2 + attn1)\n",
    "\n",
    "        ff = self.feedforward(attn2)\n",
    "        ff = self.dropout3(ff)\n",
    "        ff = self.norm3(ff + attn2)\n",
    "\n",
    "        return ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9b349c444881b030f492a713a0ade9d",
     "grade": false,
     "grade_id": "cell-586469eac0b19254",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_DecoderBlock_shapes():\n",
    "    decoder_block = DecoderBlock(n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    y = torch.tensor([\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 0],\n",
    "        [6, 0],\n",
    "    ]).float().view(4, 2, 1).repeat(1, 1, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    z = torch.randn(4, 2, 16, requires_grad=True)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "    tgt_mask = subsequent_mask(y.size(0))\n",
    "\n",
    "    outputs = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    assert outputs.shape == torch.Size([4, 2, 16]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_DecoderBlock_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69e0dab83d63ad8412e15478e6d7b781",
     "grade": true,
     "grade_id": "test_DecoderBlock",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs1[:2,1,:]:\n",
      " tensor([[-0.4045,  0.7694, -0.4516, -1.8077, -0.3729, -0.4025,  1.3268,  0.3066,\n",
      "          1.9091,  0.5253, -0.8343,  1.7696, -0.8863, -0.5096, -0.9071, -0.0304],\n",
      "        [-0.2468,  0.6168, -0.4144, -1.9228, -0.2940, -0.4228,  1.5256,  0.3525,\n",
      "          1.9966,  0.3821, -0.7561,  1.6048, -0.8995, -0.4497, -0.7678, -0.3046]])\n",
      "outputs2[:2,1,:]:\n",
      " tensor([[-0.4045,  0.7694, -0.4516, -1.8077, -0.3729, -0.4025,  1.3268,  0.3066,\n",
      "          1.9091,  0.5253, -0.8343,  1.7696, -0.8863, -0.5096, -0.9071, -0.0304],\n",
      "        [-0.1525,  0.5036, -0.3144, -1.9850, -0.2585, -0.4252,  1.5904,  0.3166,\n",
      "          2.0350,  0.3591, -0.7220,  1.5193, -0.9259, -0.4327, -0.7104, -0.3975]])\n",
      "outputs3[:2,1,:]:\n",
      " tensor([[-0.4045,  0.7694, -0.4516, -1.8077, -0.3729, -0.4025,  1.3268,  0.3066,\n",
      "          1.9091,  0.5253, -0.8343,  1.7696, -0.8863, -0.5096, -0.9071, -0.0304],\n",
      "        [-0.1525,  0.5036, -0.3144, -1.9850, -0.2585, -0.4252,  1.5904,  0.3166,\n",
      "          2.0350,  0.3591, -0.7220,  1.5193, -0.9259, -0.4327, -0.7104, -0.3975]])\n",
      "outputs4[:2,1,:]:\n",
      " tensor([[-0.2380,  0.8402, -0.3543, -1.5771, -0.1803, -0.0190,  1.0149,  0.6563,\n",
      "          2.0901,  0.1169, -1.2686,  1.6444, -0.6352, -0.4812, -1.3074, -0.3015],\n",
      "        [-0.0142,  0.5835, -0.2507, -1.7215, -0.0873, -0.0438,  1.2493,  0.6935,\n",
      "          2.2292, -0.0312, -1.1664,  1.3995, -0.6694, -0.4028, -1.1279, -0.6399]])\n",
      "outputs5[:2,1,:]:\n",
      " tensor([[-0.2380,  0.8402, -0.3543, -1.5771, -0.1803, -0.0190,  1.0149,  0.6563,\n",
      "          2.0901,  0.1169, -1.2686,  1.6444, -0.6352, -0.4812, -1.3074, -0.3015],\n",
      "        [-0.0142,  0.5835, -0.2507, -1.7215, -0.0873, -0.0438,  1.2493,  0.6935,\n",
      "          2.2292, -0.0312, -1.1664,  1.3995, -0.6694, -0.4028, -1.1279, -0.6399]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests DecoderBlock\n",
    "# Check that the signal does not propagate from the padded elements of the source sequence and\n",
    "# subsequent elements of the target sequence.\n",
    "@unittest.mock.patch('torch.nn.functional.dropout', no_dropout)  # .eval() does not disable F.dropout()\n",
    "def test_DecoderBlock():\n",
    "    with torch.no_grad():\n",
    "        decoder_block = DecoderBlock(n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "        y = torch.tensor([\n",
    "            [1, 2],\n",
    "            [3, 4],\n",
    "            [5, 0],\n",
    "            [6, 0],\n",
    "        ]).float().view(4, 2, 1).repeat(1, 1, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "        z = torch.randn(4, 2, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "        src_mask = torch.tensor([\n",
    "            [0, 0],\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [0, 1],\n",
    "        ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "        tgt_mask = subsequent_mask(y.size(0))\n",
    "\n",
    "        outputs1 = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        assert outputs1.shape == torch.Size([4, 2, 16]), f\"Wrong outputs1.shape: {outputs1.shape}\"\n",
    "        print('outputs1[:2,1,:]:\\n', outputs1[:2,1,:])\n",
    "\n",
    "        # Modify second element of y[1]\n",
    "        y[1,1,:] = y[1,1,:] + 1\n",
    "        outputs2 = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        print('outputs2[:2,1,:]:\\n', outputs2[:2,1,:])\n",
    "        diff = outputs2[1,1,:] - outputs1[1,1,:]\n",
    "        assert torch.max(torch.abs(diff)) > 0, \"Some elements of the target sequence do not affect the output.\"\n",
    "        diff = outputs2[0,1,:] - outputs1[0,1,:]\n",
    "        assert torch.max(torch.abs(diff)) == 0, \"Subsequent elements of the target sequence affect the output.\"\n",
    "\n",
    "        # Modify padded values of y\n",
    "        y[2:,1,:] = y[2:,1,:] + 1\n",
    "        outputs3 = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        print('outputs3[:2,1,:]:\\n', outputs3[:2,1,:])\n",
    "        diff = outputs3[:2,1,:] - outputs2[:2,1,:]\n",
    "        assert torch.max(torch.abs(diff)) == 0, \"Padding values in the target sequence affect the output.\"\n",
    "        \n",
    "        # Modify non-padded values of z\n",
    "        z[:2,1,:] = z[:2,1,:] + 1\n",
    "        outputs4 = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        print('outputs4[:2,1,:]:\\n', outputs4[:2,1,:])\n",
    "        diff = outputs4[:2,1,:] - outputs3[:2,1,:]\n",
    "        assert torch.max(torch.abs(diff)) > 0, \"Some elements of the source sequence do not affect the output.\"\n",
    "\n",
    "        # Modify padded values of y\n",
    "        z[2:,1,:] = z[2:,1,:] + 1\n",
    "        outputs5 = decoder_block(y, z, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        print('outputs5[:2,1,:]:\\n', outputs5[:2,1,:])\n",
    "        diff = outputs5[:2,1,:] - outputs4[:2,1,:]\n",
    "        assert torch.max(torch.abs(diff)) == 0, \"Padding values in the source sequence affect the output.\"\n",
    "\n",
    "        print('Success')\n",
    "\n",
    "test_DecoderBlock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75e526bbcb79bce524570c1f5fab827d",
     "grade": false,
     "grade_id": "cell-a30448f3b22189c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"decoder.png\" width=200 style=\"float: right;\">\n",
    "\n",
    "## Decoder\n",
    "\n",
    "The decoder is a stack of the following blocks:\n",
    "* Embedding of words (please use [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding))\n",
    "* Positional encoding (please use `tr.PositionalEncoding` from the attached module)\n",
    "* `n_blocks` of the `DecoderBlock` modules.\n",
    "* A linear layer with `tgt_vocab_size` output features.\n",
    "* Log_softmax nonlinearity.\n",
    "\n",
    "Note: our longest sequences have length `MAX_LENGTH`, this is the value that you can use when you specify `PositionalEncoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1b8877ee3226b22ab4145c196d6f50d",
     "grade": false,
     "grade_id": "Decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, n_blocks, n_features, n_heads, n_hidden=64, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt_vocab_size: Number of words in the target vocabulary.\n",
    "          n_blocks: Number of EncoderBlock blocks.\n",
    "          n_features: Number of features to be used for word embedding and further in all layers of the decoder.\n",
    "          n_heads: Number of attention heads inside the DecoderBlock.\n",
    "          n_hidden: Number of hidden units in the Feedforward block of DecoderBlock.\n",
    "          dropout: Dropout level used in DecoderBlock.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, n_features)\n",
    "        self.positional = tr.PositionalEncoding(n_features, dropout, MAX_LENGTH)\n",
    "        module = DecoderBlock(n_features, n_heads, n_hidden, dropout)\n",
    "        self.blocks = nn.ModuleList([copy.deepcopy(module) for _ in range(n_blocks)])\n",
    "        self.linear = nn.Linear(n_features, tgt_vocab_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, y, z, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y of shape (max_tgt_seq_length, batch_size): LongTensor with the target sequences.\n",
    "          z of shape (max_src_seq_length, batch_size, n_features): Encoded source sequences (outputs of the\n",
    "              encoder).\n",
    "          src_mask of shape (max_src_seq_length, batch_size): Boolean tensor indicating which elements of the\n",
    "             source sequences should be ignored.\n",
    "        \n",
    "        Returns:\n",
    "          out of shape (max_seq_length, batch_size, tgt_vocab_size): Log-softmax probabilities of the words\n",
    "              in the output sequences.\n",
    "\n",
    "        Notes:\n",
    "          * All intermediate signals should be of shape (max_seq_length, batch_size, n_features).\n",
    "          * You need to create and use the subsequent mask in the decoder.\n",
    "        \"\"\"\n",
    "        mask = subsequent_mask(y.size(0))\n",
    "        mask = mask.to(device)\n",
    "        y = self.embedding(y)\n",
    "        y = self.positional(y)\n",
    "        for block in self.blocks:\n",
    "            y = block(y, z, src_mask=src_mask, tgt_mask=mask)\n",
    "        y = self.linear(y)\n",
    "        y = self.log_softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51bb46bd5ecbe26f8645207b8f0ff89b",
     "grade": false,
     "grade_id": "cell-2511aa618604b4a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Decoder_shapes():\n",
    "    decoder = Decoder(tgt_vocab_size=10, n_blocks=1, n_features=16, n_heads=4, n_hidden=64)\n",
    "\n",
    "    y = torch.tensor([\n",
    "        [SOS_token,     SOS_token],\n",
    "        [        3,             4],\n",
    "        [        5,     EOS_token],\n",
    "        [        6, PADDING_VALUE],\n",
    "        [        7, PADDING_VALUE],\n",
    "    ])  # (max_seq_length, batch_size)\n",
    "\n",
    "    z = torch.randn(5, 2, 16)  # (max_seq_length, batch_size, n_features)\n",
    "\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "    outputs = decoder(y, z, src_mask=src_mask)\n",
    "    assert outputs.shape == torch.Size([5, 2, 10]), f\"Wrong outputs.shape: {outputs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac142e2969764fa1e646091bf0255a7a",
     "grade": false,
     "grade_id": "cell-10c28584fb58b386",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21ab3ba0364f3ec7e43d3204c702e831",
     "grade": false,
     "grade_id": "cell-ec608760ebfa7f8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(2925, 256)\n",
       "  (positional): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-2): 3 x DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (src_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (feedforward): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=2925, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the transformer model\n",
    "n_features = 256\n",
    "encoder = Encoder(src_vocab_size=trainset.input_lang.n_words, n_blocks=3, n_features=n_features,\n",
    "                  n_heads=16, n_hidden=1024)\n",
    "decoder = Decoder(tgt_vocab_size=trainset.output_lang.n_words, n_blocks=3, n_features=n_features,\n",
    "                  n_heads=16, n_hidden=1024)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7452e92ad959e13ecdf0e16354afc06",
     "grade": false,
     "grade_id": "cell-b1645c0797a9d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "In the training loop, we first encode source sequences using the encoder. Then we decode the encoded sequences by the decoder which also receives shifted target sequences as inputs. The decoder outputs a tensor that contains log-softmax probabilities of words in the output language. You need to use those probabilities to compute the loss. Note that you need to ignore the padded values in the target sequences (similarly to Exercise 5).\n",
    "\n",
    "Hints:\n",
    "* The training loss should be smaller than 0.1 at the end of training.\n",
    "* If you use the `NoamOptimizer` defined below, you should reach the level of 0.1 after 15-20 epochs.\n",
    "* If you use the Adam optimizer with learning rate 0.001, you should reach the level of 0.1 after 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cc8a5cb93a4033853f0874b927adb57",
     "grade": false,
     "grade_id": "cell-3dcb8dddc9bd9ee7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "adam = torch.optim.Adam(parameters, lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = tr.NoamOptimizer(n_features, 0.4, 680, adam)\n",
    "#optimizer = torch.optim.Adam(parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(dataloader, encoder, decoder, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, mask, target_tensor = data\n",
    "        \n",
    "        input_tensor = input_tensor.to(device)\n",
    "        mask = mask.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = encoder(input_tensor, mask)\n",
    "        out = decoder(target_tensor[:-1], out, mask)\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(target_tensor[1:].size(0)):\n",
    "            loss += criterion(out[t], target_tensor[1:][t])\n",
    "        loss = loss / target_tensor[1:].size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6270848f5387bf01aba9bb5f50303a78",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss = 4.4578\n",
      "Epoch: 2, loss = 2.1505\n",
      "Epoch: 3, loss = 1.6505\n",
      "Epoch: 4, loss = 1.2868\n",
      "Epoch: 5, loss = 1.0058\n",
      "Epoch: 6, loss = 0.7538\n",
      "Epoch: 7, loss = 0.5379\n",
      "Epoch: 8, loss = 0.3821\n",
      "Epoch: 9, loss = 0.2739\n",
      "Epoch: 10, loss = 0.2009\n",
      "Epoch: 11, loss = 0.1560\n",
      "Epoch: 12, loss = 0.1287\n",
      "Epoch: 13, loss = 0.1079\n",
      "Epoch: 14, loss = 0.0941\n",
      "Epoch: 15, loss = 0.0837\n",
      "Epoch: 16, loss = 0.0772\n",
      "Epoch: 17, loss = 0.0697\n",
      "Epoch: 18, loss = 0.0632\n",
      "Epoch: 19, loss = 0.0618\n",
      "Epoch: 20, loss = 0.0602\n",
      "Training done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    # Create the transformer model\n",
    "    n_features = 256\n",
    "    encoder = Encoder(src_vocab_size=trainset.input_lang.n_words, n_blocks=3, n_features=n_features,\n",
    "                    n_heads=16, n_hidden=1024)\n",
    "    decoder = Decoder(tgt_vocab_size=trainset.output_lang.n_words, n_blocks=3, n_features=n_features,\n",
    "                    n_heads=16, n_hidden=1024)\n",
    "    device = torch.device('cuda:0')\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    plot_losses = []\n",
    "\n",
    "    parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    adam = torch.optim.Adam(parameters, lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    optimizer = tr.NoamOptimizer(n_features, 0.4, 680, adam)\n",
    "\n",
    "    epochs = 20\n",
    "    criterion = nn.NLLLoss(ignore_index = PADDING_VALUE)\n",
    "    criterion.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(trainloader, encoder, decoder, optimizer, criterion)\n",
    "        plot_losses.append(loss)\n",
    "        \n",
    "        print('Epoch: %d, loss = %.4f' % (epoch + 1, loss))\n",
    "    \n",
    "    print('Training done!')\n",
    "    plt.plot(range(epochs), plot_losses)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not saved.\n",
      "Model not saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '1_tr_encoder.pth', confirm=True)\n",
    "    tools.save_model(decoder, '1_tr_decoder.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "623dde0b23a00fea2b9ede9fe041fade",
     "grade": false,
     "grade_id": "accuracy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 1_tr_encoder.pth.\n",
      "Model loaded from 1_tr_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "if skip_training:\n",
    "    encoder = Encoder(src_vocab_size=trainset.input_lang.n_words, n_blocks=3, n_features=256, n_heads=16, n_hidden=1024)\n",
    "    tools.load_model(encoder, '1_tr_encoder.pth', device)\n",
    "    \n",
    "    decoder = Decoder(tgt_vocab_size=trainset.output_lang.n_words, n_blocks=3, n_features=256, n_heads=16, n_hidden=1024)\n",
    "    tools.load_model(decoder, '1_tr_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54788aadd640f3e28e828ecd8bac433a",
     "grade": true,
     "grade_id": "cell-985f2404fb056035",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests the trained transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbe8e80ea48ee0d2a224a9c2c1b2ac84",
     "grade": false,
     "grade_id": "cell-25e4072e5588afaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Generate translations with the trained model\n",
    "\n",
    "In the cell below, implement a function that converts an input sequence to an output sequence using the trained transformer.\n",
    "\n",
    "Notes:\n",
    "* Since we do not need to compute the gradients in the evaluation phase, we can speed up the computations by using the statement `with torch.no_grad():`.\n",
    "* Please transfer the tensors to `device` inside this function.\n",
    "* We may deduct some points for an ineffecient implementation of `translate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6e4df363d49dedc5988600dae2a0c27",
     "grade": false,
     "grade_id": "cell-870f9d3b10a20a75",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, src_seqs, src_mask):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "    \n",
    "    Args:\n",
    "      encoder (Encoder): Trained encoder.\n",
    "      decoder (Decoder): Trained decoder.\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): LongTensor of padded source sequences.\n",
    "      src_mask of shape (max_src_seq_length, batch_size): BoolTensor indicating which elements of the src_seqs\n",
    "          tensor should be ignored in computations: True values in src_mask correspond to padding values in src_seqs.\n",
    "    \n",
    "    Returns:\n",
    "      out_seqs of shape (MAX_LENGTH, batch_size): LongTensor of word indices of the output sequences.\n",
    "      \n",
    "      NOTE: The SOS token should not be included in out_seqs.\n",
    "    \"\"\"\n",
    "    device = src_seqs.device\n",
    "    batch_size = src_seqs.size(1)\n",
    "    with torch.no_grad():\n",
    "        encoder = encoder.to(device)\n",
    "        decoder = decoder.to(device)\n",
    "        src_seqs = src_seqs.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "\n",
    "        out = encoder(src_seqs, src_mask)\n",
    "        out_seqs = torch.LongTensor(1, batch_size).fill_(PADDING_VALUE).type_as(src_seqs.data).to(device)\n",
    "        out_seqs[0,:] = SOS_token\n",
    "        for i in range(MAX_LENGTH):\n",
    "          output = decoder(out_seqs, out, src_mask)\n",
    "          nextWord = output[i,:,:].argmax(dim=1)\n",
    "          out_seqs = torch.cat([out_seqs, nextWord.unsqueeze(0)], dim=0)\n",
    "        return out_seqs[1:,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58e60d452d9d0bf8612e3f3ac9a5040",
     "grade": true,
     "grade_id": "cell-ce3fec48785a469b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (max_seq_length, batch_size)\n",
    "\n",
    "    src_seqs, src_mask = src_seqs.to(device), src_mask.to(device)\n",
    "    out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "    assert not torch.any(out_seqs[0] == SOS_token), \"out_seqs should not include the SOS_token.\"\n",
    "    assert out_seqs.shape == torch.Size([MAX_LENGTH, 2]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "851357309224307e5d09e547d6ef68be",
     "grade": true,
     "grade_id": "cell-faacada7cd5bb286",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_seqs:\n",
      " tensor([[  1,   1],\n",
      "        [  2,   2],\n",
      "        [  4,   4],\n",
      "        [  8,   8],\n",
      "        [ 16,  16],\n",
      "        [ 32,  32],\n",
      "        [ 64,  64],\n",
      "        [128, 128],\n",
      "        [256, 256],\n",
      "        [512, 512]])\n",
      "expected:\n",
      " tensor([[  1,   1],\n",
      "        [  2,   2],\n",
      "        [  4,   4],\n",
      "        [  8,   8],\n",
      "        [ 16,  16],\n",
      "        [ 32,  32],\n",
      "        [ 64,  64],\n",
      "        [128, 128],\n",
      "        [256, 256],\n",
      "        [512, 512]])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests your implementation of translate()\n",
    "def test_translate():\n",
    "    src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "    src_mask = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "    ], dtype=torch.bool)  # (batch_size, max_seq_length)\n",
    "\n",
    "    class _Encoder(nn.Module):\n",
    "        def forward(self, x, mask):\n",
    "            return x.float()\n",
    "\n",
    "    class _Decoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            self._counter = 0\n",
    "            super(_Decoder, self).__init__()\n",
    "            \n",
    "        def forward(self, y, z, src_mask):\n",
    "            vocab_size = trainset.output_lang.n_words\n",
    "            ix = torch.minimum(y.cumsum(dim=0).long() + 1, torch.full_like(y, (vocab_size - 1)))\n",
    "            #ix = y.cumsum(dim=0).long() + 1\n",
    "            out = torch.zeros(y.size(0), y.size(1), vocab_size)\n",
    "            for i in range(y.size(1)):\n",
    "                out[torch.arange(y.size(0)), i, ix[:, i]] = 1\n",
    "            return out\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder = _Encoder()\n",
    "        decoder = _Decoder()\n",
    "        out_seqs = translate(encoder, decoder, src_seqs, src_mask).cpu()\n",
    "    print('out_seqs:\\n', out_seqs)\n",
    "    expected = torch.tensor([\n",
    "        [  1,   1],\n",
    "        [  2,   2],\n",
    "        [  4,   4],\n",
    "        [  8,   8],\n",
    "        [ 16,  16],\n",
    "        [ 32,  32],\n",
    "        [ 64,  64],\n",
    "        [128, 128],\n",
    "        [256, 256],\n",
    "        [512, 512]])\n",
    "    print('expected:\\n', expected)\n",
    "    assert (out_seqs == expected).all(), \"out_seqs does not match expected value.\"\n",
    "    TRANSLATE_IS_CORRECT = True\n",
    "    print('Success')\n",
    "    \n",
    "test_translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06bbbbacf38af8dcd371f4f8d90a3494",
     "grade": true,
     "grade_id": "cell-bbd806e753b38b5a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_seqs:\n",
      " tensor([[  1,   1],\n",
      "        [  2,   2],\n",
      "        [  4,   4],\n",
      "        [  8,   8],\n",
      "        [ 16,  16],\n",
      "        [ 32,  32],\n",
      "        [ 64,  64],\n",
      "        [128, 128],\n",
      "        [256, 256],\n",
      "        [512, 512]])\n",
      "expected:\n",
      " tensor([[  1,   1],\n",
      "        [  2,   2],\n",
      "        [  4,   4],\n",
      "        [  8,   8],\n",
      "        [ 16,  16],\n",
      "        [ 32,  32],\n",
      "        [ 64,  64],\n",
      "        [128, 128],\n",
      "        [256, 256],\n",
      "        [512, 512]])\n",
      "Success\n",
      "Time: 0.3749089241027832\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell tests your implementation of translate()\n",
    "try:\n",
    "    test_translate()\n",
    "except:\n",
    "    raise 'translate() function should correctly implemented to get points for speed.'\n",
    "\n",
    "import time\n",
    "def test_translate_speed():\n",
    "    start_time = time.time()\n",
    "    for src_seqs, src_mask, tgt_seqs in trainloader:\n",
    "        out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "        break\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {end_time - start_time}')\n",
    "    assert end_time - start_time < 1, \"Too slow implementation. You must have an unnecessary for-loop in your code.\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_speed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a7476bd986bb058cb138bfba26f2f92",
     "grade": false,
     "grade_id": "cell-b57f4854a969cb31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, we translate sentences from the training set. For a well-trained transformer, the translations should look similar to the target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "303eed1d5a4185b59a3489fe57755342",
     "grade": false,
     "grade_id": "cell-a8ba1c2f1b1173fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def seq_to_tokens(seq, lang):\n",
    "    'Convert a sequence of word indices into a list of words (strings).'\n",
    "    sentence = []\n",
    "    for i in seq:\n",
    "        if i == EOS_token:\n",
    "            break\n",
    "        sentence.append(lang.index2word[i.item()])\n",
    "    return(sentence)\n",
    "\n",
    "def seq_to_string(seq, lang):\n",
    "    'Convert a sequence of word indices into a sentence string.'\n",
    "    return(' '.join(seq_to_tokens(seq, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d72930240f694f36eaea6d80605f70a4",
     "grade": true,
     "grade_id": "cell-fb6fdac69a71a0c9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate training data:\n",
      "-----------------------------\n",
      "SRC: vous etes tellement previsibles !\n",
      "TGT: you re so predictable .\n",
      "OUT: you re so predictable .\n",
      "\n",
      "SRC: c est un vieil ami a moi .\n",
      "TGT: he is an old friend of mine .\n",
      "OUT: he is an old friend of mine .\n",
      "\n",
      "SRC: vous etes epuisees .\n",
      "TGT: you re exhausted .\n",
      "OUT: you re exhausted .\n",
      "\n",
      "SRC: vous etes un de ces pauvres types !\n",
      "TGT: you re such a jerk .\n",
      "OUT: you re such a jerk .\n",
      "\n",
      "SRC: tu as parfaitement raison .\n",
      "TGT: you re perfectly right .\n",
      "OUT: you re perfectly right .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Translate training data:')\n",
    "print('-----------------------------')\n",
    "src_seqs, src_mask, tgt_seqs = next(iter(trainloader))\n",
    "out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(src_seqs[:,i], trainset.input_lang))\n",
    "    print('TGT:', seq_to_string(tgt_seqs[1:,i], trainset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], trainset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1d5f46da25dace4601c3cc25ef206da",
     "grade": false,
     "grade_id": "cell-67752c380a50b644",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, we translate sentences from the test set. For a well-trained transformer, the translations are typically\n",
    "worse than for the training sequences but they still look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45143560c78f29599468ee0f359a4f38",
     "grade": true,
     "grade_id": "cell-6b598b0fd1633b71",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate test data:\n",
      "-----------------------------\n",
      "SRC: je suis juste une fille normale .\n",
      "TGT: i m just an average girl .\n",
      "OUT: i m just a normal girl .\n",
      "\n",
      "SRC: il est toujours en train de se plaindre .\n",
      "TGT: he is constantly complaining .\n",
      "OUT: he is always complaining .\n",
      "\n",
      "SRC: je vous suis reconnaissant de votre gentillesse .\n",
      "TGT: i am grateful to you for your kindness .\n",
      "OUT: i am grateful to you about your kindness .\n",
      "\n",
      "SRC: je suis impatiente de te voir danser .\n",
      "TGT: i m looking forward to seeing you dance .\n",
      "OUT: i m looking forward to seeing you dance .\n",
      "\n",
      "SRC: nous sommes endurantes .\n",
      "TGT: we re resilient .\n",
      "OUT: we re resilient .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Translate test data:')\n",
    "print('-----------------------------')\n",
    "src_seqs, src_mask, tgt_seqs = next(iter(testloader))\n",
    "out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(src_seqs[:,i], testset.input_lang))\n",
    "    print('TGT:', seq_to_string(tgt_seqs[1:,i], testset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], testset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21cef34c8dd81d76d85a2ad72105dac9",
     "grade": false,
     "grade_id": "cell-67c9d1b917f1f917",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Compute BLEU score\n",
    "\n",
    "Let us now compute the [BLEU score](https://en.wikipedia.org/wiki/BLEU) for the translations produced by our model. We can use the PyTorch function [bleu_score](https://pytorch.org/text/stable/data_metrics.html?highlight=bleu_score#torchtext.data.metrics.bleu_score) for that.\n",
    "\n",
    "* **Your model should achieve a minimum BLEU score of 92 on the training set.**\n",
    "* The BLEU score on the test set should be greater than 55.\n",
    "\n",
    "The model can severly overfit to the training set and we do not cope with the overfitting problem in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd58221014bd67cbf6347fd0b4d19a00",
     "grade": false,
     "grade_id": "cell-5a0ad89516fd02e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    # Create translations for the training set\n",
    "    candidate_corpus = []\n",
    "    references_corpus = []\n",
    "    for src_seqs, src_mask, tgt_seqs in trainloader:\n",
    "        out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "        candidate_corpus.extend([seq_to_tokens(seq, trainset.output_lang) for seq in out_seqs.T])\n",
    "        references_corpus.extend([[seq_to_tokens(seq, trainset.output_lang)] for seq in tgt_seqs[1:].T])\n",
    "\n",
    "    # Compute BLEU for translations\n",
    "    score = bleu_score(candidate_corpus, references_corpus)\n",
    "    print(f'BLEU score on training data: {score*100}')\n",
    "    assert score*100 > 90, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc511949cc558ce496877f0485edfea8",
     "grade": true,
     "grade_id": "cell-fa389a3327f82c0d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 94.64402794837952\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This cell is reserved for grading\n",
    "def test_bleu():\n",
    "    # Create translations for the training set\n",
    "    candidate_corpus = []\n",
    "    references_corpus = []\n",
    "    for i, (src_seqs, src_mask, tgt_seqs) in enumerate(trainloader):\n",
    "        out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "        candidate_corpus.extend([seq_to_tokens(seq, trainset.output_lang) for seq in out_seqs.T])\n",
    "        references_corpus.extend([[seq_to_tokens(seq, trainset.output_lang)] for seq in tgt_seqs[1:].T])\n",
    "        if i == 3:\n",
    "            break  # Use only 4 batches for testing\n",
    "\n",
    "    # Compute BLEU for translations\n",
    "    score = bleu_score(candidate_corpus, references_corpus)\n",
    "    print(f'BLEU score on training data: {score*100}')\n",
    "    assert score*100 > 92, \"The BLEU score is too low.\"\n",
    "    print('Success')\n",
    "\n",
    "test_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dbb1bbb7ec0069a7a507568a11435a8",
     "grade": true,
     "grade_id": "cell-49d0466ea4a34ba0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on test data: 59.46074724197388\n"
     ]
    }
   ],
   "source": [
    "# Create translations for the test set\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "for i, (src_seqs, src_mask, tgt_seqs) in enumerate(testloader):\n",
    "    out_seqs = translate(encoder, decoder, src_seqs, src_mask)\n",
    "    candidate_corpus.extend([seq_to_tokens(seq, testset.output_lang) for seq in out_seqs.T])\n",
    "    references_corpus.extend([[seq_to_tokens(seq, testset.output_lang)] for seq in tgt_seqs[1:].T])\n",
    "    if i == 1:\n",
    "        break  # Use only 2 batches for testing\n",
    "\n",
    "# Compute BLEU for translations\n",
    "score = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score on test data: {score*100}')\n",
    "assert score*100 > 55, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c25aadf68f7f6983cd16a664ee1f44b7",
     "grade": false,
     "grade_id": "cell-14e56d480d46ea14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook:\n",
    "* We trained a transformer-based sequence-to-sequence model for statistical machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
